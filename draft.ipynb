{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36940476-07b5-476f-803d-7feefaa7541a",
   "metadata": {},
   "source": [
    "# Data pre-processing for EEG seizure detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac8131d-094d-44ed-a115-93945f12898e",
   "metadata": {},
   "source": [
    "The dataset is made available as multiple SMRX files generated by Spike2 analysis software. `neo` package can be used to read the data. \n",
    "There are 6 labelled seizures in the data, timestamps and associated filenames of each seizures are stored in a `list` of `dict`s for further processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367cd3e6-5b4a-4653-8e18-7ea5f81ff01f",
   "metadata": {},
   "source": [
    "<span style=\"color:orange\">TODO : Generate larger dataset with random non-seizure sequences. Use SMOTE to balance the classes.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09e1c3bc-fd8b-4f24-acc0-6c5eaefc7301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seizure data structure\n",
    "seizures = [\n",
    "    {\n",
    "        'filename': 'SCN-AJ_180910_183823_1001',\n",
    "        'sequence': (13539, 13580),\n",
    "        'padded_sequence': (13519, 13600),\n",
    "    },\n",
    "    {\n",
    "        'filename': 'SCN-AJ_180911_063946_1003',\n",
    "        'sequence': (21040, 21094),\n",
    "        'padded_sequence': (21013, 21121),\n",
    "    },\n",
    "    {\n",
    "        'filename': 'SCN-AJ_180912_004151_1006',\n",
    "        'sequence': (8284, 8325 ),\n",
    "        'padded_sequence': (8264, 8345),\n",
    "    },\n",
    "    {\n",
    "        'filename': 'SCN-AJ_180912_064236_1007',\n",
    "        'sequence': (9918, 9956),\n",
    "        'padded_sequence': (9900, 9975),\n",
    "    },\n",
    "    {\n",
    "        'filename': 'SCN-AJ_180912_064236_1007',\n",
    "        'sequence': (11865, 11914),\n",
    "        'padded_sequence': (11840, 11940),\n",
    "    },\n",
    "    {\n",
    "        'filename': 'SCN-AJ_180912_184406_1009',\n",
    "        'sequence': (1, 24),\n",
    "        'padded_sequence': (1, 50),\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b840038-47cd-498e-86d2-88568fcdddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neo\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import random\n",
    "\n",
    "def generateLearningData(seizures, signal_length, sampling_rate=3000):\n",
    "    '''\n",
    "    Generates the learning dataset by combining the labelled seizures and\n",
    "    adjacent padding to create a balanced dataset\n",
    "    '''\n",
    "    \n",
    "    data = np.empty((signal_length * sampling_rate, 15))\n",
    "    labels = np.zeros(signal_length * sampling_rate)\n",
    "    sample_start = 0\n",
    "    \n",
    "    random.shuffle(seizures)\n",
    "    \n",
    "    for seizure in seizures:\n",
    "        reader = neo.io.CedIO(filename='data/' + seizure['filename'] + '.smrx')\n",
    "        segment = reader.read_segment(time_slice=None, lazy=True) # lazy loading to save memory\n",
    "        signals = segment.analogsignals\n",
    "        \n",
    "        seq_start = seizure['padded_sequence'][0]\n",
    "        seq_end = seizure['padded_sequence'][1]\n",
    "        \n",
    "        for timestep in range(seq_start, seq_end - 1):\n",
    "            \n",
    "            sample_end = sample_start + sampling_rate\n",
    "            \n",
    "            for channel, signal in enumerate(signals):\n",
    "                signal_slice = signal.load(time_slice=(timestep, timestep + 1))\n",
    "                signal_slice = sp.signal.resample(signal_slice, sampling_rate)\n",
    "                \n",
    "                sample = sample_start\n",
    "                for slice_index in range(sampling_rate):\n",
    "                    data[sample, channel] = signal_slice[slice_index]\n",
    "                    sample += 1\n",
    "                    \n",
    "            # generating labels\n",
    "            if timestep >= seizure['sequence'][0] and timestep + 1 <= seizure['sequence'][1]:\n",
    "                labels[sample_start:sample_end] = 1\n",
    "                    \n",
    "            sample_start = sample_end\n",
    "\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "340f9751-9b1f-42e4-b8aa-8f4057dee011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1485000, 15)\n",
      "(1485000,)\n"
     ]
    }
   ],
   "source": [
    "raw_data, labels = generateLearningData(seizures, 495) # 495s is the total length of seizures + padding\n",
    "print(raw_data.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "024b11c9-b404-4939-b54a-660095706ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: Counter({0.0: 747000, 1.0: 738000})\n"
     ]
    }
   ],
   "source": [
    "# verify the class ratio\n",
    "from collections import Counter\n",
    "print(\"Original class distribution: %s\" % Counter(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f0aa6d7-d05a-485b-a28f-d4440be1b3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the scales of each channel\n",
    "data = raw_data\n",
    "for channel in range(15):\n",
    "    data[:, channel] = (data[:, channel] - np.min(data[:, channel])) / (np.max(data[:, channel]) - np.min(data[:, channel]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa5c2918-f317-476b-a9bc-f1024dc1a121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data sequence for LSTM\n",
    "def generateDataSequqnce(data, labels, sequence_length, step):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for start in range(0, len(data), step):\n",
    "        end = start + sequence_length\n",
    "        X.append(data[start:end])\n",
    "        Y.append(labels[start])\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2700ab5-8b2a-4df9-a6ba-971dec48a008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1485000, 1, 15)\n",
      "(1485000,)\n"
     ]
    }
   ],
   "source": [
    "sequence_length, step = 1, 1\n",
    "X, Y = generateDataSequqnce(data, labels, sequence_length, step)\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7301307a-970d-4465-aef5-fe135cdee058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (1189485, 1, 15)\n",
      "y_train (1189485,)\n",
      "x_val (147015, 1, 15)\n",
      "y_val (147015,)\n",
      "x_test (148500, 1, 15)\n",
      "y_test (148500,)\n"
     ]
    }
   ],
   "source": [
    "# train:val:test split 80:10:10\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, x_test, train_labels, y_test = train_test_split(X, Y, test_size=0.1, shuffle=False)\n",
    "x_train, x_val, y_train, y_val = train_test_split(train_data, train_labels, test_size=0.11, shuffle=False)\n",
    "\n",
    "y_train = np.asarray(y_train).astype('float32')\n",
    "y_val = np.asarray(y_val).astype('float32')\n",
    "y_test = np.asarray(y_test).astype('float32')\n",
    "\n",
    "print(\"x_train:\", x_train.shape)\n",
    "print(\"y_train\", y_train.shape)\n",
    "print(\"x_val\", x_val.shape)\n",
    "print(\"y_val\", y_val.shape)\n",
    "print(\"x_test\", x_test.shape)\n",
    "print(\"y_test\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d6a448-901f-4b85-8be0-f10a8ee00313",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Baseline classification\n",
    "Fitting the model on normalized data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585dc002-07c5-4c0c-868b-a1d52fa79e54",
   "metadata": {},
   "source": [
    "<span style=\"color:orange\">TODO : Hyperparameter optimization and trying atleast one another model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b90ed593-e5e0-48ea-bbdd-1495188248e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_1 (LSTM)               (None, 16)                2048      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,065\n",
      "Trainable params: 2,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# LSTM model definition for classification\n",
    "import keras\n",
    "from keras.layers import LSTM, Dropout, Dense\n",
    "model = keras.Sequential()\n",
    "model.add(LSTM(16, input_shape = (1, 15)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(loss=\"binary_crossentropy\", \n",
    "              metrics=[keras.metrics.binary_accuracy],\n",
    "              optimizer=\"adam\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89d4d2df-b6a1-4698-8552-9dc28a8e09c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18586/18586 [==============================] - 22s 1ms/step - loss: 0.0597 - binary_accuracy: 0.9792 - val_loss: 1.0359 - val_binary_accuracy: 0.8576\n",
      "Epoch 2/10\n",
      "18586/18586 [==============================] - 22s 1ms/step - loss: 0.0588 - binary_accuracy: 0.9795 - val_loss: 0.8127 - val_binary_accuracy: 0.8756\n",
      "Epoch 3/10\n",
      "18586/18586 [==============================] - 22s 1ms/step - loss: 0.0583 - binary_accuracy: 0.9796 - val_loss: 0.7480 - val_binary_accuracy: 0.8794\n",
      "Epoch 4/10\n",
      "18586/18586 [==============================] - 22s 1ms/step - loss: 0.0578 - binary_accuracy: 0.9801 - val_loss: 0.7463 - val_binary_accuracy: 0.8802\n",
      "Epoch 5/10\n",
      "18586/18586 [==============================] - 22s 1ms/step - loss: 0.0567 - binary_accuracy: 0.9804 - val_loss: 0.6349 - val_binary_accuracy: 0.8930\n",
      "Epoch 6/10\n",
      "18586/18586 [==============================] - 22s 1ms/step - loss: 0.0563 - binary_accuracy: 0.9807 - val_loss: 0.5465 - val_binary_accuracy: 0.9021\n",
      "Epoch 7/10\n",
      "18586/18586 [==============================] - 22s 1ms/step - loss: 0.0558 - binary_accuracy: 0.9807 - val_loss: 0.6597 - val_binary_accuracy: 0.8903\n",
      "Epoch 8/10\n",
      "18586/18586 [==============================] - 22s 1ms/step - loss: 0.0555 - binary_accuracy: 0.9809 - val_loss: 0.4658 - val_binary_accuracy: 0.9151\n",
      "Epoch 9/10\n",
      "18586/18586 [==============================] - 22s 1ms/step - loss: 0.0548 - binary_accuracy: 0.9812 - val_loss: 0.4977 - val_binary_accuracy: 0.9126\n",
      "Epoch 10/10\n",
      "18586/18586 [==============================] - 22s 1ms/step - loss: 0.0538 - binary_accuracy: 0.9814 - val_loss: 0.5327 - val_binary_accuracy: 0.9054\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fce63f033a0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2)\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=64, epochs=10, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f75e13da-4f02-4aea-8dd7-e842e682e815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4641/4641 [==============================] - 3s 629us/step - loss: 10.2993 - binary_accuracy: 0.5723\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[10.299306869506836, 0.5722693800926208]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.evaluate(x_test, y_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c50846a-8e0e-4cfc-a178-f527d3d27966",
   "metadata": {},
   "source": [
    "The model overfits and generalizes poorly on unseen test data, although has good validation performance. Very little hyperparameter optimization was performed for the model, which will be done after fixing the issue with the Butterworth bandpass filter. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b93aff-40c5-4ea0-ac01-669f75b1ba10",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "The aim is to apply two sets of pre-processing methodologies and compare their impacts - frequency-domain methods based on signal processing, and extracting statistical features from the data to be used as features. Feature selection will be done initially to reduce the dimensionality of the data and reduce computational requirements for working with larger subsets of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c53a7fb-94cc-477b-af81-dc0632692d81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
